{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaPreWRDCYFOqlpLdanDXs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcBAGo3BrnuP"},"outputs":[],"source":["!pip install librosa matplotlib numpy pillow\n","!pip uninstall pyarrow -y\n","!pip install --upgrade pyarrow datasets\n","!pip install datasets\n","!pip install evaluate\n","!pip install accelerate -U"]},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageEnhance, ImageOps\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","import random"],"metadata":{"id":"Mxo1latpsWSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define paths for the dataset\n","path_to_data = '/kaggle/input/spectrograms-data/Spectrograms'\n","hc_folder = os.path.join(path_to_data, 'HC_AH')  # Healthy audio samples\n","pd_folder = os.path.join(path_to_data, 'PD_AH')  # Parkinson's audio samples\n","\n","# Paths for saving spectrograms in Google Drive\n","spectrogram_hc_folder = '/kaggle/input/spectrograms-data/Spectrograms/healthy'\n","spectrogram_pd_folder = '/kaggle/input/spectrograms-data/Spectrograms/parkinson'\n","\n","# Create directories if they do not exist\n","os.makedirs(spectrogram_hc_folder, exist_ok=True)\n","os.makedirs(spectrogram_pd_folder, exist_ok=True)"],"metadata":{"id":"VW75fLwEsWUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_spectrogram(audio_path, save_folder, file_name, chunk_size=0.05, sample_rate=22050, limit = 0):\n","    # Load the audio file\n","    y, sr = librosa.load(audio_path, sr=sample_rate)\n","\n","    # Divide into chunks (0.1 seconds)\n","    chunk_length = int(chunk_size * sr)  # Convert chunk size (in seconds) to samples\n","    total_chunks = len(y) // chunk_length\n","\n","    if limit == 0:\n","        limit = total_chunks\n","\n","    for i in range(limit):\n","        # Get the chunk of audio\n","        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n","\n","        # Create a spectrogram using STFT\n","        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64,n_fft=256, hop_length=64)\n","        S_DB = librosa.power_to_db(S, ref=np.max)\n","\n","        # Plot the spectrogram and save it as an image\n","        plt.figure(figsize=(2, 2))\n","        plt.axis('off')  # Remove axes\n","\n","        librosa.display.specshow(S_DB, sr=sr, cmap='viridis')\n","        save_path = os.path.join(save_folder, f'{file_name}_chunk_{i}.png')\n","        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n","        plt.close()"],"metadata":{"id":"XcIe89RrsWWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_spectrogram_augmented(y, sr, chunk_size=0.1, sample_rate=22050, limit=0):\n","    \"\"\"\n","    Create spectrograms from audio chunks and return them as 224x224 RGB images.\n","\n","    Args:\n","        y (np.ndarray): Audio time-series.\n","        sr (int): Sampling rate of the audio.\n","        chunk_size (float): Length of each chunk in seconds.\n","        sample_rate (int): Target sampling rate for processing.\n","        limit (int): Maximum number of chunks to process. If 0, process all chunks.\n","\n","    Returns:\n","        List[Image]: List of PIL Image objects containing spectrograms.\n","    \"\"\"\n","    chunk_length = int(chunk_size * sr)  # Convert chunk size (in seconds) to samples\n","    total_chunks = len(y) // chunk_length\n","\n","    if limit == 0:\n","        limit = total_chunks\n","    else:\n","        limit = min(limit, total_chunks)\n","\n","    spectrogram_images = []\n","\n","    for i in range(limit):\n","        # Get the chunk of audio\n","        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n","\n","        # Create a spectrogram using STFT\n","        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64, n_fft=256, hop_length=64)\n","        S_DB = librosa.power_to_db(S, ref=np.max)\n","\n","        # Plot the spectrogram\n","        fig, ax = plt.subplots(figsize=(3.2, 3.2), dpi=72)  # 3.2 * 72 = 224 pixels\n","        ax.axis('off')  # Remove axes\n","\n","        # librosa.display.specshow(S_DB, sr=sr, cmap='viridis', ax=ax)\n","\n","        # Convert plot to image\n","        fig.canvas.draw()\n","        img = np.array(fig.canvas.renderer.buffer_rgba())  # Get RGBA image\n","\n","        # Convert to PIL Image and ensure RGB format\n","        img_pil = Image.fromarray(img).convert('RGB')\n","        img_pil = img_pil.resize((224, 224), Image.LANCZOS)  # Ensure exact size\n","\n","        plt.close(fig)  # Close the figure to free memory\n","\n","        spectrogram_images.append(img_pil)\n","\n","    return spectrogram_images"],"metadata":{"id":"BAsxMo1zsWX6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to check if folder is empty\n","def is_folder_empty(folder_path):\n","    # Check if the folder exists and is non-empty\n","    return len(os.listdir(folder_path)) == 0\n","\n","# Loop through folders and create spectrograms if the folder is empty\n","for folder, label, save_folder in zip([hc_folder, pd_folder], ['healthy', 'parkinson'], [spectrogram_hc_folder, spectrogram_pd_folder]):\n","    if is_folder_empty(save_folder):\n","        print(f\"Generating spectrograms for {label} data...\")\n","        for file in os.listdir(folder):\n","            if file.endswith('.wav'):  # Ensure it's an audio file\n","                file_path = os.path.join(folder, file)\n","                create_spectrogram(file_path, save_folder, file_name=os.path.splitext(file)[0])\n","    else:\n","        print(f\"Spectrogram folder for {label} data already exists and is not empty. Skipping generation.\")"],"metadata":{"id":"QfDW7HmUsWZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to apply random augmentation to an image\n","def apply_random_augmentation(img):\n","    # Random rotation\n","    if random.random() < 0.5:\n","        img = img.rotate(random.choice([0, 90, 180, 270]))\n","\n","    # Random horizontal flip\n","    if random.random() < 0.5:\n","        img = ImageOps.mirror(img)\n","\n","    # Random brightness adjustment\n","    if random.random() < 0.5:\n","        enhancer = ImageEnhance.Brightness(img)\n","        img = enhancer.enhance(random.uniform(0.8, 1.2))\n","\n","    return img"],"metadata":{"id":"_wxRSVIYsWbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_time_domain_augmentation(audio_path, shift_max=0.2, stretch_factor=1.2):\n","    \"\"\"\n","    Apply time-domain augmentations (time shifting and time stretching) to an audio sample.\n","\n","    Args:\n","        audio_path (str): Path to the input audio file.\n","        output_path (str): Path to save the augmented audio.\n","        shift_max (float): Maximum fraction of the total duration to shift (e.g., 0.2 for 20%).\n","        stretch_factor (float): Factor by which to stretch the time (e.g., 1.2 to increase speed by 20%).\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Load the audio file\n","    y, sr = librosa.load(audio_path, sr=None)\n","\n","    # Apply time shifting\n","    shift_samples = int(shift_max * len(y))  # Number of samples to shift\n","    shift = np.random.randint(-shift_samples, shift_samples)\n","    y_shifted = np.roll(y, shift)\n","\n","    # Apply time stretching\n","    y_stretched = librosa.effects.time_stretch(y_shifted, rate=stretch_factor)\n","\n","    # Ensure the stretched audio matches the original length by trimming or padding\n","    if len(y_stretched) > len(y):\n","        y_stretched = y_stretched[:len(y)]\n","    else:\n","        y_stretched = np.pad(y_stretched, (0, len(y) - len(y_stretched)))\n","\n","    return y_stretched"],"metadata":{"id":"ZI_ndY0ZsWc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count initial number of images without setting target_count\n","def load_initial_images(spectrogram_folder, label):\n","    images = []\n","    labels = []\n","    for file in os.listdir(spectrogram_folder):\n","        if file.endswith('.png'):\n","            image_path = os.path.join(spectrogram_folder, file)\n","            img = Image.open(image_path).convert('RGB')  # Convert to RGB\n","            img = img.resize((224, 224))  # Resize the image to 224x224\n","            images.append(np.array(img))\n","            labels.append(label)\n","    return images, labels\n","\n","# Load initial datasets without augmentation to determine counts\n","healthy_images, healthy_labels = load_initial_images(spectrogram_hc_folder, 'healthy')\n","parkinson_images, parkinson_labels = load_initial_images(spectrogram_pd_folder, 'parkinson')\n","\n","# Set target_count as the maximum count between the two categories\n","target_count = min(len(healthy_images), len(parkinson_images))\n","\n","# Reload with augmentation to ensure balanced dataset\n","def load_dataset_with_limit(audio_folder, spectrogram_folder, label, target_count):\n","    images, labels = load_initial_images(spectrogram_folder, label)\n","    # Randomly select target_count samples\n","    selected_indices = random.sample(range(len(images)), target_count)\n","    images = [images[i] for i in selected_indices]\n","    labels = [labels[i] for i in selected_indices]\n","\n","    return images, labels\n","\n","print(\"Count : \"+str(target_count))"],"metadata":{"id":"wLpshdQMsWeU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use data augmentation to balance both categories to target_count\n","healthy_images, healthy_labels = load_dataset_with_limit(hc_folder,spectrogram_hc_folder, 'healthy', target_count)\n","print(len(healthy_images))\n","parkinson_images, parkinson_labels = load_dataset_with_limit(pd_folder,spectrogram_pd_folder, 'parkinson', target_count)\n","print(len(parkinson_images))\n","\n","# Combine datasets\n","images = healthy_images + parkinson_images\n","labels = healthy_labels + parkinson_labels\n","\n","# Split the data into train and test sets\n","train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","# Create Hugging Face dataset structure\n","train_data = {\"image\": train_images, \"label\": train_labels}\n","test_data = {\"image\": test_images, \"label\": test_labels}\n","\n","# Convert to Hugging Face Dataset\n","train_dataset = Dataset.from_dict(train_data)\n","test_dataset = Dataset.from_dict(test_data)\n","\n","# Combine into a DatasetDict\n","dataset = DatasetDict({\n","    'train': train_dataset,\n","    'validation': test_dataset\n","})\n","\n","print(dataset)"],"metadata":{"id":"iIMZ2J3_sWfu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import torch\n","from PIL import Image\n","from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n","from tqdm import tqdm\n","from evaluate import load\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","import accelerate\n","import transformers\n","\n","device = \"cpu\"\n","\n","model_name = \"google/vit-base-patch16-224\"                        # the model name\n","image_processor = ViTImageProcessor.from_pretrained(model_name)   # load the image processor\n","modelViT = ViTForImageClassification.from_pretrained(model_name)     # loading the pre-trained model\n","\n","labels = ['healthy', 'parkinson']\n","\n","label_map = {'healthy': 0, 'parkinson': 1}"],"metadata":{"id":"9Ke0k23qsWhN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    # Split the batch into images (X) and labels (y)\n","    images = [item['image'] for item in batch]  # First element in each item is the image\n","    labels = [item['label'] for item in batch]  # Second element in each item is the label\n","\n","    # Convert images to tensors\n","    images_tensor = torch.stack([torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in images])  # Convert images to tensors and change dimension order\n","\n","    # Convert labels from strings ('healthy', 'parkinson') to integers (0, 1)\n","    labels_tensor = torch.tensor([label_map[label] for label in labels], dtype=torch.long)\n","\n","    return {\n","        'pixel_values': images_tensor,  # Image tensor\n","        'labels': labels_tensor         # Label tensor (numerical)\n","    }"],"metadata":{"id":"qT1WOQK-sWlk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the accuracy and f1 metrics from the evaluate module\n","accuracy = load(\"accuracy\")\n","f1 = load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    # Predictions and true labels\n","    predictions = eval_pred.predictions\n","    references = eval_pred.label_ids\n","\n","    # Compute accuracy and F1 scores\n","    accuracy_score = accuracy.compute(predictions=np.argmax(predictions, axis=1), references=references)\n","    f1_score = f1.compute(predictions=np.argmax(predictions, axis=1), references=references, average=\"macro\")\n","\n","    # Compute AUC (for multiclass classification, use 'ovo' or 'ovr')\n","    try:\n","        auc_score = roc_auc_score(references, predictions, multi_class=\"ovr\", average=\"macro\")\n","    except ValueError:\n","        # Handle cases where AUC cannot be computed (e.g., single-class predictions)\n","        auc_score = np.nan\n","\n","    # Plot the ROC curve\n","    plt.figure(figsize=(8, 6))\n","    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n","        # Multiclass ROC curve\n","        for i in range(predictions.shape[1]):\n","            fpr, tpr, _ = roc_curve(references == i, predictions[:, i])\n","            plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc_score(references == i, predictions[:, i]):.2f})\")\n","    else:\n","        # Binary ROC curve\n","        fpr, tpr, _ = roc_curve(references, predictions[:, 1])\n","        plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n","\n","    # Configure plot\n","    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC Curve\")\n","    plt.legend(loc=\"lower right\")\n","    plt.grid()\n","    plt.show()\n","\n","    # Return all metrics\n","    return {**accuracy_score, **f1_score, \"auc\": auc_score}"],"metadata":{"id":"vHWfg8_bs9X7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the ViT model\n","modelViT = ViTForImageClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(labels),\n","    id2label={str(i): c for i, c in enumerate(labels)},\n","    label2id={c: str(i) for i, c in enumerate(labels)},\n","    ignore_mismatched_sizes=True,\n",")\n","\n","# defining the training arguments\n","training_args = TrainingArguments(\n","  output_dir=\"/kaggle/working/vit-base\", # output directory\n","  per_device_train_batch_size=32, # batch size per device during training\n","  eval_strategy=\"steps\",    # evaluation strategy to adopt during training\n","  num_train_epochs=20,             # total number of training epochs\n","  # fp16=True,                    # use mixed precision\n","  save_steps=80,                # number of update steps before saving checkpoint\n","  eval_steps=80,                # number of update steps before evaluating\n","  logging_steps=80,             # number of update steps before logging\n","  save_total_limit=2,             # limit the total amount of checkpoints on disk\n","  remove_unused_columns=False,    # remove unused columns from the dataset\n","  push_to_hub=False,              # do not push the model to the hub\n","  report_to='tensorboard',        # report metrics to tensorboard\n","  load_best_model_at_end=True,    # load the best model at the end of training\n",")"],"metadata":{"id":"Ddnm2VNgtBMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainerViT = Trainer(\n","    model=modelViT,                        # the instantiated ðŸ¤— Transformers model to be trained\n","    args=training_args,                 # training arguments, defined above\n","    data_collator=collate_fn,           # the data collator that will be used for batching\n","    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n","    train_dataset=dataset[\"train\"],     # training dataset\n","    eval_dataset=dataset[\"validation\"], # evaluation dataset\n","    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",")\n","\n","trainerViT.train()\n","\n","torch.save(modelViT.state_dict(), '/kaggle/working/ViT_model.pth')"],"metadata":{"id":"1m7tbkIVtD5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ResNet\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification, TrainingArguments, Trainer\n","from datasets import load_dataset\n","import numpy as np\n","from evaluate import load\n","import requests\n","import torch\n","from PIL import Image\n","from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n","from tqdm import tqdm\n","from evaluate import load\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","import accelerate\n","import transformers"],"metadata":{"id":"H20Y-5WktGes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = ['healthy', 'parkinson']\n","\n","label_map = {'healthy': 0, 'parkinson': 1}\n","\n","def collate_fn(batch):\n","    # Split the batch into images (X) and labels (y)\n","    images = [item['image'] for item in batch]  # First element in each item is the image\n","    labels = [item['label'] for item in batch]  # Second element in each item is the label\n","\n","    # Convert images to tensors\n","    images_tensor = torch.stack([torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in images])  # Convert images to tensors and change dimension order\n","\n","    # Convert labels from strings ('healthy', 'parkinson') to integers (0, 1)\n","    labels_tensor = torch.tensor([label_map[label] for label in labels], dtype=torch.long)\n","\n","    return {\n","        'pixel_values': images_tensor,  # Image tensor\n","        'labels': labels_tensor         # Label tensor (numerical)\n","    }"],"metadata":{"id":"Sq_8shw5tJjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the accuracy and f1 metrics from the evaluate module\n","accuracy = load(\"accuracy\")\n","f1 = load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    # Predictions and true labels\n","    predictions = eval_pred.predictions\n","    references = eval_pred.label_ids\n","\n","    # Compute accuracy and F1 scores\n","    accuracy_score = accuracy.compute(predictions=np.argmax(predictions, axis=1), references=references)\n","    f1_score = f1.compute(predictions=np.argmax(predictions, axis=1), references=references, average=\"macro\")\n","\n","    # Compute AUC (for multiclass classification, use 'ovo' or 'ovr')\n","    try:\n","        auc_score = roc_auc_score(references, predictions, multi_class=\"ovr\", average=\"macro\")\n","    except ValueError:\n","        # Handle cases where AUC cannot be computed (e.g., single-class predictions)\n","        auc_score = np.nan\n","\n","    # Plot the ROC curve\n","    plt.figure(figsize=(8, 6))\n","    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n","        # Multiclass ROC curve\n","        for i in range(predictions.shape[1]):\n","            fpr, tpr, _ = roc_curve(references == i, predictions[:, i])\n","            plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc_score(references == i, predictions[:, i]):.2f})\")\n","    else:\n","            # Binary ROC curve\n","        fpr, tpr, _ = roc_curve(references, predictions[:, 1])\n","        plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n","\n","    # Configure plot\n","    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC Curve\")\n","    plt.legend(loc=\"lower right\")\n","    plt.grid()\n","    plt.show()\n","\n","    # Return all metrics\n","    return {**accuracy_score, **f1_score, \"auc\": auc_score}"],"metadata":{"id":"Ks6KdNEbtMDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelResNet = ResNetForImageClassification.from_pretrained(\n","    'microsoft/resnet-50',\n","    num_labels=2,  # Binary classification\n","    ignore_mismatched_sizes=True\n",")\n","\n","# training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\", # output directory\n","  per_device_train_batch_size=32, # batch size per device during training\n","  eval_strategy=\"steps\",    # evaluation strategy to adopt during training\n","  num_train_epochs=20,             # total number of training epochs\n","  save_steps=80,                # number of update steps before saving checkpoint\n","  eval_steps=80,                # number of update steps before evaluating\n","  logging_steps=80,             # number of update steps before logging\n","  save_total_limit=2,             # limit the total amount of checkpoints on disk\n","  remove_unused_columns=False,    # remove unused columns from the dataset\n","  push_to_hub=False,\n","report_to='tensorboard',        # report metrics to tensorboard\n","  load_best_model_at_end=True,\n",")"],"metadata":{"id":"7Z5_iR_TtRfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading a pre-trained ResNet feature extractor\n","feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-50')\n","\n","# Initializing the Trainer\n","trainerResNet = Trainer(\n","    model=modelResNet,\n","    args=training_args,\n","    data_collator=collate_fn,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"validation\"],\n","    tokenizer=feature_extractor,  # Use the feature extractor here\n","    compute_metrics=compute_metrics\n",")\n","\n","trainerResNet.train()\n","\n","torch.save(modelResNet.state_dict(), '/kaggle/working/ResNet_model.pth')"],"metadata":{"id":"gx7j590MtWwh"},"execution_count":null,"outputs":[]}]}