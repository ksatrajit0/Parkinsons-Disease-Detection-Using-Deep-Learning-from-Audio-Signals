{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO63DI7rIEuBhNEUSzSdq8d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MfPFqX6Fnq-i"},"outputs":[],"source":["!pip install librosa matplotlib numpy pillow\n","!pip uninstall pyarrow -y\n","!pip install --upgrade pyarrow datasets\n","!pip install datasets\n","!pip install evaluate\n","!pip install accelerate -U"]},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageEnhance, ImageOps\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","import random"],"metadata":{"id":"k8OecYXRn0qB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define paths for the dataset\n","path_to_data = '/kaggle/input/spectrograms-data/Spectrograms'\n","hc_folder = os.path.join(path_to_data, 'HC_AH')  # Healthy audio samples\n","pd_folder = os.path.join(path_to_data, 'PD_AH')  # Parkinson's audio samples\n","\n","# Paths for saving spectrograms in Google Drive\n","spectrogram_hc_folder = '/kaggle/input/spectrograms-data/Spectrograms/healthy'\n","spectrogram_pd_folder = '/kaggle/input/spectrograms-data/Spectrograms/parkinson'\n","\n","# Create directories if they do not exist\n","os.makedirs(spectrogram_hc_folder, exist_ok=True)\n","os.makedirs(spectrogram_pd_folder, exist_ok=True)"],"metadata":{"id":"-2efGyEXn0s3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_spectrogram(audio_path, save_folder, file_name, chunk_size=0.05, sample_rate=22050, limit = 0):\n","    # Load the audio file\n","    y, sr = librosa.load(audio_path, sr=sample_rate)\n","\n","    # Divide into chunks (0.1 seconds)\n","    chunk_length = int(chunk_size * sr)  # Convert chunk size (in seconds) to samples\n","    total_chunks = len(y) // chunk_length\n","\n","    if limit == 0:\n","        limit = total_chunks\n","\n","    for i in range(limit):\n","        # Get the chunk of audio\n","        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n","\n","        # Create a spectrogram using STFT\n","        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64,n_fft=256, hop_length=64)\n","        S_DB = librosa.power_to_db(S, ref=np.max)\n","\n","        # Plot the spectrogram and save it as an image\n","        plt.figure(figsize=(2, 2))\n","        plt.axis('off')  # Remove axes\n","\n","        librosa.display.specshow(S_DB, sr=sr, cmap='viridis')\n","        save_path = os.path.join(save_folder, f'{file_name}_chunk_{i}.png')\n","        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n","        plt.close()"],"metadata":{"id":"sJScIM5dn0vT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_spectrogram_augmented(y, sr, chunk_size=0.1, sample_rate=22050, limit=0):\n","    \"\"\"\n","    Create spectrograms from audio chunks and return them as 224x224 RGB images.\n","\n","    Args:\n","        y (np.ndarray): Audio time-series.\n","        sr (int): Sampling rate of the audio.\n","        chunk_size (float): Length of each chunk in seconds.\n","        sample_rate (int): Target sampling rate for processing.\n","        limit (int): Maximum number of chunks to process. If 0, process all chunks.\n","\n","    Returns:\n","        List[Image]: List of PIL Image objects containing spectrograms.\n","    \"\"\"\n","    chunk_length = int(chunk_size * sr)  # Convert chunk size (in seconds) to samples\n","    total_chunks = len(y) // chunk_length\n","\n","    if limit == 0:\n","        limit = total_chunks\n","    else:\n","        limit = min(limit, total_chunks)\n","\n","    spectrogram_images = []\n","\n","    for i in range(limit):\n","        # Get the chunk of audio\n","        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n","\n","        # Create a spectrogram using STFT\n","        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64, n_fft=256, hop_length=64)\n","        S_DB = librosa.power_to_db(S, ref=np.max)\n","\n","        # Plot the spectrogram\n","        fig, ax = plt.subplots(figsize=(3.2, 3.2), dpi=72)  # 3.2 * 72 = 224 pixels\n","        ax.axis('off')  # Remove axes\n","\n","        # librosa.display.specshow(S_DB, sr=sr, cmap='viridis', ax=ax)\n","\n","        # Convert plot to image\n","        fig.canvas.draw()\n","        img = np.array(fig.canvas.renderer.buffer_rgba())  # Get RGBA image\n","\n","        # Convert to PIL Image and ensure RGB format\n","        img_pil = Image.fromarray(img).convert('RGB')\n","        img_pil = img_pil.resize((224, 224), Image.LANCZOS)  # Ensure exact size\n","\n","        plt.close(fig)  # Close the figure to free memory\n","\n","        spectrogram_images.append(img_pil)\n","\n","    return spectrogram_images"],"metadata":{"id":"_vJBRx7Hn0xa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to check if folder is empty\n","def is_folder_empty(folder_path):\n","    # Check if the folder exists and is non-empty\n","    return len(os.listdir(folder_path)) == 0\n","\n","# Loop through folders and create spectrograms if the folder is empty\n","for folder, label, save_folder in zip([hc_folder, pd_folder], ['healthy', 'parkinson'], [spectrogram_hc_folder, spectrogram_pd_folder]):\n","    if is_folder_empty(save_folder):\n","        print(f\"Generating spectrograms for {label} data...\")\n","        for file in os.listdir(folder):\n","            if file.endswith('.wav'):  # Ensure it's an audio file\n","                file_path = os.path.join(folder, file)\n","                create_spectrogram(file_path, save_folder, file_name=os.path.splitext(file)[0])\n","    else:\n","        print(f\"Spectrogram folder for {label} data already exists and is not empty. Skipping generation.\")"],"metadata":{"id":"IrBwTvFqn0z1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to apply random augmentation to an image\n","def apply_random_augmentation(img):\n","    # Random rotation\n","    if random.random() < 0.5:\n","        img = img.rotate(random.choice([0, 90, 180, 270]))\n","\n","    # Random horizontal flip\n","    if random.random() < 0.5:\n","        img = ImageOps.mirror(img)\n","\n","    # Random brightness adjustment\n","    if random.random() < 0.5:\n","        enhancer = ImageEnhance.Brightness(img)\n","        img = enhancer.enhance(random.uniform(0.8, 1.2))\n","\n","    return img"],"metadata":{"id":"nBS1Jougn04Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_time_domain_augmentation(audio_path, shift_max=0.2, stretch_factor=1.2):\n","    \"\"\"\n","    Apply time-domain augmentations (time shifting and time stretching) to an audio sample.\n","\n","    Args:\n","        audio_path (str): Path to the input audio file.\n","        output_path (str): Path to save the augmented audio.\n","        shift_max (float): Maximum fraction of the total duration to shift (e.g., 0.2 for 20%).\n","        stretch_factor (float): Factor by which to stretch the time (e.g., 1.2 to increase speed by 20%).\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Load the audio file\n","    y, sr = librosa.load(audio_path, sr=None)\n","\n","    # Apply time shifting\n","    shift_samples = int(shift_max * len(y))  # Number of samples to shift\n","    shift = np.random.randint(-shift_samples, shift_samples)\n","    y_shifted = np.roll(y, shift)\n","\n","    # Apply time stretching\n","    y_stretched = librosa.effects.time_stretch(y_shifted, rate=stretch_factor)\n","\n","    # Ensure the stretched audio matches the original length by trimming or padding\n","    if len(y_stretched) > len(y):\n","        y_stretched = y_stretched[:len(y)]\n","    else:\n","        y_stretched = np.pad(y_stretched, (0, len(y) - len(y_stretched)))\n","\n","    return y_stretched"],"metadata":{"id":"ME5vkbh0oG33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count initial number of images without setting target_count\n","def load_initial_images(spectrogram_folder, label):\n","    images = []\n","    labels = []\n","    for file in os.listdir(spectrogram_folder):\n","        if file.endswith('.png'):\n","            image_path = os.path.join(spectrogram_folder, file)\n","            img = Image.open(image_path).convert('RGB')  # Convert to RGB\n","            img = img.resize((224, 224))  # Resize the image to 224x224\n","            images.append(np.array(img))\n","            labels.append(label)\n","    return images, labels"],"metadata":{"id":"yjABpEsFoLCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load initial datasets without augmentation to determine counts\n","healthy_images, healthy_labels = load_initial_images(spectrogram_hc_folder, 'healthy')\n","parkinson_images, parkinson_labels = load_initial_images(spectrogram_pd_folder, 'parkinson')\n","\n","# Set target_count as the maximum count between the two categories\n","target_count = min(len(healthy_images), len(parkinson_images))\n","\n","# Reload with augmentation to ensure balanced dataset\n","def load_dataset_with_limit(audio_folder, spectrogram_folder, label, target_count):\n","    images, labels = load_initial_images(spectrogram_folder, label)\n","    # Randomly select target_count samples\n","    selected_indices = random.sample(range(len(images)), target_count)\n","    images = [images[i] for i in selected_indices]\n","    labels = [labels[i] for i in selected_indices]\n","\n","    return images, labels\n","\n","print(\"Count : \"+str(target_count))\n","\n","# Use data augmentation to balance both categories to target_count\n","healthy_images, healthy_labels = load_dataset_with_limit(hc_folder,spectrogram_hc_folder, 'healthy', target_count)\n","print(len(healthy_images))\n","parkinson_images, parkinson_labels = load_dataset_with_limit(pd_folder,spectrogram_pd_folder, 'parkinson', target_count)\n","print(len(parkinson_images))\n","\n","# Combine datasets\n","images = healthy_images + parkinson_images\n","labels = healthy_labels + parkinson_labels"],"metadata":{"id":"3XY4s-teoN8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into train and test sets\n","train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","# Create Hugging Face dataset structure\n","train_data = {\"image\": train_images, \"label\": train_labels}\n","test_data = {\"image\": test_images, \"label\": test_labels}\n","\n","# Convert to Hugging Face Dataset\n","train_dataset = Dataset.from_dict(train_data)\n","test_dataset = Dataset.from_dict(test_data)\n","\n","# Combine into a DatasetDict\n","dataset = DatasetDict({\n","    'train': train_dataset,\n","    'validation': test_dataset\n","})\n","\n","print(dataset)"],"metadata":{"id":"EzuQdWtooRuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ViTForImageClassification\n","import torch\n","\n","labels = ['healthy', 'parkinson']\n","\n","# Re-initialize the model architecture with correct label mappings\n","modelViT_loaded = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224\",\n","    num_labels=2,\n","    id2label={str(i): c for i, c in enumerate(labels)},\n","    label2id={c: str(i) for i, c in enumerate(labels)},\n","    ignore_mismatched_sizes=True,\n",")\n","\n","# Load the saved weights\n","modelViT_loaded.load_state_dict(torch.load(\"/kaggle/input/vit_parkinson/pytorch/default/1/ViT_model.pth\", map_location=torch.device(\"cpu\")))"],"metadata":{"id":"qxjplL-goY7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","from transformers import AutoFeatureExtractor\n","\n","# Load a pre-trained feature extractor for MobileNetV2\n","feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224), antialias=True),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n","])"],"metadata":{"id":"71fTvcqUocTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ResNetForImageClassification\n","\n","# Recreate the model architecture\n","modelResNet_loaded = ResNetForImageClassification.from_pretrained(\n","    'microsoft/resnet-50',\n","    num_labels=2,\n","    ignore_mismatched_sizes=True,\n",")\n","\n","# Load the saved weights into the model\n","modelResNet_loaded.load_state_dict(torch.load(\"/kaggle/input/resnet_parkinson/pytorch/default/1/ResNet_model.pth\", map_location=torch.device(\"cpu\")))"],"metadata":{"id":"foAzVc4Voe3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import MobileNetV2ForImageClassification\n","\n","# Re-initialize the model architecture\n","modelMobileNet_loaded = MobileNetV2ForImageClassification.from_pretrained(\n","    \"google/mobilenet_v2_1.0_224\",\n","    num_labels=2,\n","    ignore_mismatched_sizes=True\n",")\n","\n","# Load the state_dict\n","modelMobileNet_loaded.load_state_dict(torch.load(\"/kaggle/input/mobilenet_parkinson/pytorch/default/1/MobileNet_model.pth\", map_location=torch.device(\"cpu\")))"],"metadata":{"id":"C6fAj5O8ohlE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","modelResNet_loaded = modelResNet_loaded.to(device)\n","modelViT_loaded = modelViT_loaded.to(device)\n","modelMobileNet_loaded = modelMobileNet_loaded.to(device)\n","\n","basic_transform = transforms.ToTensor()"],"metadata":{"id":"KdzR39nCok7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_ensemble(image):\n","    x1 = transform(image).unsqueeze(0).to(device)\n","    x = basic_transform(image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        r = modelResNet_loaded(x)\n","        v = modelViT_loaded(x)\n","        m = modelMobileNet_loaded(x1)\n","\n","    # Average predictions\n","    avg_logits = (r.logits + v.logits + m.logits) / 3\n","    pred = torch.argmax(avg_logits, dim=1).item()\n","\n","    return pred"],"metadata":{"id":"zZCBufCEorI2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","predictions = []\n","true_labels = []\n","\n","for example in tqdm(dataset[\"validation\"]):\n","    raw_image = example['image']\n","\n","    # Ensure image is in HWC format for PIL conversion\n","    if isinstance(raw_image, list):\n","        raw_image = np.array(raw_image, dtype=np.uint8)\n","\n","    if isinstance(raw_image, torch.Tensor):\n","        raw_image = raw_image.permute(1, 2, 0).numpy()  # CHW → HWC\n","\n","    if len(raw_image.shape) == 2:  # Grayscale\n","        raw_image = np.stack([raw_image] * 3, axis=-1)\n","    elif len(raw_image.shape) == 3 and raw_image.shape[0] == 3:  # CHW\n","        raw_image = np.transpose(raw_image, (1, 2, 0))  # Convert to HWC\n","\n","    pil_image = Image.fromarray(raw_image.astype(np.uint8))\n","\n","    # Don't transform here. Let predict_ensemble handle it.\n","    pred = predict_ensemble(pil_image)\n","\n","    predictions.append(pred)\n","    if example['label'] == \"healthy\":\n","        true_labels.append(0)\n","    else:\n","        true_labels.append(1)"],"metadata":{"id":"tmmBfqz9osDT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, accuracy_score, f1_score\n","print(true_labels[0])\n","print(predictions[0])\n","print(classification_report(true_labels, predictions, target_names=[\"healthy\", \"parkinson\"]))\n","print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n","print(\"F1 Score:\",f1_score(true_labels,predictions))"],"metadata":{"id":"-rF13B3gowkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_ensemble_hardvote(image):\n","    x1 = transform(image).unsqueeze(0).to(device)\n","    x = basic_transform(image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        # Get predicted class from each model (0 or 1)\n","        r_pred = torch.argmax(modelResNet_loaded(x).logits, dim=1).item()\n","        v_pred = torch.argmax(modelViT_loaded(x).logits, dim=1).item()\n","        m_pred = torch.argmax(modelMobileNet_loaded(x1).logits, dim=1).item()\n","\n","    # Majority voting\n","    preds = [r_pred, v_pred, m_pred]\n","    final_pred = round(sum(preds) / len(preds))  # returns 1 if 2 or more are 1, else 0\n","\n","    return final_pred"],"metadata":{"id":"-I2gA2IVoxgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","predictions = []\n","true_labels = []\n","\n","for example in tqdm(dataset[\"validation\"]):\n","    raw_image = example['image']\n","\n","    # Ensure image is in HWC format for PIL conversion\n","    if isinstance(raw_image, list):\n","        raw_image = np.array(raw_image, dtype=np.uint8)\n","\n","    if isinstance(raw_image, torch.Tensor):\n","        raw_image = raw_image.permute(1, 2, 0).numpy()  # CHW → HWC\n","\n","    if len(raw_image.shape) == 2:  # Grayscale\n","        raw_image = np.stack([raw_image] * 3, axis=-1)\n","    elif len(raw_image.shape) == 3 and raw_image.shape[0] == 3:  # CHW\n","        raw_image = np.transpose(raw_image, (1, 2, 0))  # Convert to HWC\n","\n","    pil_image = Image.fromarray(raw_image.astype(np.uint8))\n","\n","    # Don't transform here. Let predict_ensemble handle it.\n","    pred = predict_ensemble_hardvote(pil_image)\n","\n","    predictions.append(pred)\n","    if example['label'] == \"healthy\":\n","        true_labels.append(0)\n","    else:\n","        true_labels.append(1)"],"metadata":{"id":"nFWeCHHhoz1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# testing using hardVoting\n","from sklearn.metrics import classification_report, accuracy_score, f1_score\n","print(true_labels[0])\n","print(predictions[0])\n","print(classification_report(true_labels, predictions, target_names=[\"healthy\", \"parkinson\"]))\n","print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n","print(\"F1 Score:\",f1_score(true_labels,predictions))"],"metadata":{"id":"HImV-PihpEyE"},"execution_count":null,"outputs":[]}]}