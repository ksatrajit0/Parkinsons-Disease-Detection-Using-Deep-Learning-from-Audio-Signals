{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHed+Wcl//PgczIaDsUZdu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Nbc-cuZWpj6k"},"outputs":[],"source":["!pip install librosa matplotlib numpy pillow\n","!pip uninstall pyarrow -y\n","!pip install --upgrade pyarrow datasets\n","!pip install datasets\n","!pip install evaluate\n","!pip install accelerate -U\n","!pip install --upgrade transformers"]},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image, ImageEnhance, ImageOps\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","import random"],"metadata":{"id":"qFMBNPn3pvoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define paths for the dataset\n","path_to_data = '/kaggle/input/spectrograms-data/Spectrograms'\n","hc_folder = os.path.join(path_to_data, 'HC_AH')  # Healthy audio samples\n","pd_folder = os.path.join(path_to_data, 'PD_AH')  # Parkinson's audio samples\n","\n","# Paths for saving spectrograms in Google Drive\n","spectrogram_hc_folder = '/kaggle/input/spectrograms-data/Spectrograms/healthy'\n","spectrogram_pd_folder = '/kaggle/input/spectrograms-data/Spectrograms/parkinson'\n","\n","# Create directories if they do not exist\n","os.makedirs(spectrogram_hc_folder, exist_ok=True)\n","os.makedirs(spectrogram_pd_folder, exist_ok=True)"],"metadata":{"id":"aetGvZiJpvp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_spectrogram(audio_path, save_folder, file_name, chunk_size=0.05, sample_rate=22050, limit = 0):\n","    # Load the audio file\n","    y, sr = librosa.load(audio_path, sr=sample_rate)\n","\n","    # Divide into chunks (0.1 seconds)\n","    chunk_length = int(chunk_size * sr)  # Convert chunk size (in seconds) to samples\n","    total_chunks = len(y) // chunk_length\n","\n","    if limit == 0:\n","        limit = total_chunks\n","\n","    for i in range(limit):\n","        # Get the chunk of audio\n","        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n","\n","        # Create a spectrogram using STFT\n","        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64,n_fft=256, hop_length=64)\n","        S_DB = librosa.power_to_db(S, ref=np.max)\n","\n","        # Plot the spectrogram and save it as an image\n","        plt.figure(figsize=(2, 2))\n","        plt.axis('off')  # Remove axes\n","\n","        librosa.display.specshow(S_DB, sr=sr, cmap='viridis')\n","        save_path = os.path.join(save_folder, f'{file_name}_chunk_{i}.png')\n","        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n","        plt.close()"],"metadata":{"id":"D3HxhhihpvsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_spectrogram_augmented(y, sr, chunk_size=0.1, sample_rate=22050, limit=0):\n","    \"\"\"\n","    Create spectrograms from audio chunks and return them as 224x224 RGB images.\n","\n","    Args:\n","        y (np.ndarray): Audio time-series.\n","        sr (int): Sampling rate of the audio.\n","        chunk_size (float): Length of each chunk in seconds.\n","        sample_rate (int): Target sampling rate for processing.\n","        limit (int): Maximum number of chunks to process. If 0, process all chunks.\n","\n","    Returns:\n","        List[Image]: List of PIL Image objects containing spectrograms.\n","    \"\"\"\n","    chunk_length = int(chunk_size * sr)  # Convert chunk size (in seconds) to samples\n","    total_chunks = len(y) // chunk_length\n","\n","    if limit == 0:\n","        limit = total_chunks\n","    else:\n","        limit = min(limit, total_chunks)\n","\n","    spectrogram_images = []\n","\n","    for i in range(limit):\n","        # Get the chunk of audio\n","        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n","\n","        # Create a spectrogram using STFT\n","        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64, n_fft=256, hop_length=64)\n","        S_DB = librosa.power_to_db(S, ref=np.max)\n","\n","        # Plot the spectrogram\n","        fig, ax = plt.subplots(figsize=(3.2, 3.2), dpi=72)  # 3.2 * 72 = 224 pixels\n","        ax.axis('off')  # Remove axes\n","\n","        # librosa.display.specshow(S_DB, sr=sr, cmap='viridis', ax=ax)\n","\n","        # Convert plot to image\n","        fig.canvas.draw()\n","        img = np.array(fig.canvas.renderer.buffer_rgba())  # Get RGBA image\n","\n","        # Convert to PIL Image and ensure RGB format\n","        img_pil = Image.fromarray(img).convert('RGB')\n","        img_pil = img_pil.resize((224, 224), Image.LANCZOS)  # Ensure exact size\n","\n","        plt.close(fig)  # Close the figure to free memory\n","\n","        spectrogram_images.append(img_pil)\n","\n","    return spectrogram_images"],"metadata":{"id":"lFQ97XPlpvtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to check if folder is empty\n","def is_folder_empty(folder_path):\n","    # Check if the folder exists and is non-empty\n","    return len(os.listdir(folder_path)) == 0\n","\n","# Loop through folders and create spectrograms if the folder is empty\n","for folder, label, save_folder in zip([hc_folder, pd_folder], ['healthy', 'parkinson'], [spectrogram_hc_folder, spectrogram_pd_folder]):\n","    if is_folder_empty(save_folder):\n","        print(f\"Generating spectrograms for {label} data...\")\n","        for file in os.listdir(folder):\n","            if file.endswith('.wav'):  # Ensure it's an audio file\n","                file_path = os.path.join(folder, file)\n","                create_spectrogram(file_path, save_folder, file_name=os.path.splitext(file)[0])\n","    else:\n","        print(f\"Spectrogram folder for {label} data already exists and is not empty. Skipping generation.\")"],"metadata":{"id":"eXo2qmWLpvvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to apply random augmentation to an image\n","def apply_random_augmentation(img):\n","    # Random rotation\n","    if random.random() < 0.5:\n","        img = img.rotate(random.choice([0, 90, 180, 270]))\n","\n","    # Random horizontal flip\n","    if random.random() < 0.5:\n","        img = ImageOps.mirror(img)\n","\n","    # Random brightness adjustment\n","    if random.random() < 0.5:\n","        enhancer = ImageEnhance.Brightness(img)\n","        img = enhancer.enhance(random.uniform(0.8, 1.2))\n","\n","    return img"],"metadata":{"id":"XQAcZJ0qpvxJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_time_domain_augmentation(audio_path, shift_max=0.2, stretch_factor=1.2):\n","    \"\"\"\n","    Apply time-domain augmentations (time shifting and time stretching) to an audio sample.\n","\n","    Args:\n","        audio_path (str): Path to the input audio file.\n","        output_path (str): Path to save the augmented audio.\n","        shift_max (float): Maximum fraction of the total duration to shift (e.g., 0.2 for 20%).\n","        stretch_factor (float): Factor by which to stretch the time (e.g., 1.2 to increase speed by 20%).\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Load the audio file\n","    y, sr = librosa.load(audio_path, sr=None)\n","\n","    # Apply time shifting\n","    shift_samples = int(shift_max * len(y))  # Number of samples to shift\n","    shift = np.random.randint(-shift_samples, shift_samples)\n","    y_shifted = np.roll(y, shift)\n","\n","    # Apply time stretching\n","    y_stretched = librosa.effects.time_stretch(y_shifted, rate=stretch_factor)\n","\n","    # Ensure the stretched audio matches the original length by trimming or padding\n","    if len(y_stretched) > len(y):\n","        y_stretched = y_stretched[:len(y)]\n","    else:\n","        y_stretched = np.pad(y_stretched, (0, len(y) - len(y_stretched)))\n","\n","    return y_stretched"],"metadata":{"id":"e7-FHCcPpvy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count initial number of images without setting target_count\n","def load_initial_images(spectrogram_folder, label):\n","    images = []\n","    labels = []\n","    for file in os.listdir(spectrogram_folder):\n","        if file.endswith('.png'):\n","            image_path = os.path.join(spectrogram_folder, file)\n","            img = Image.open(image_path).convert('RGB')  # Convert to RGB\n","            img = img.resize((224, 224))  # Resize the image to 224x224\n","            images.append(np.array(img))\n","            labels.append(label)\n","    return images, labels"],"metadata":{"id":"aBnEWhwupv0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load initial datasets without augmentation to determine counts\n","healthy_images, healthy_labels = load_initial_images(spectrogram_hc_folder, 'healthy')\n","parkinson_images, parkinson_labels = load_initial_images(spectrogram_pd_folder, 'parkinson')\n","\n","# Set target_count as the maximum count between the two categories\n","target_count = min(len(healthy_images), len(parkinson_images))\n","\n","# Reload with augmentation to ensure balanced dataset\n","def load_dataset_with_limit(audio_folder, spectrogram_folder, label, target_count):\n","    images, labels = load_initial_images(spectrogram_folder, label)\n","    # Randomly select target_count samples\n","    selected_indices = random.sample(range(len(images)), target_count)\n","    images = [images[i] for i in selected_indices]\n","    labels = [labels[i] for i in selected_indices]\n","\n","    return images, labels\n","\n","print(\"Count : \"+str(target_count))"],"metadata":{"id":"RjPRLeJcpv2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use data augmentation to balance both categories to target_count\n","healthy_images, healthy_labels = load_dataset_with_limit(hc_folder,spectrogram_hc_folder, 'healthy', target_count)\n","print(len(healthy_images))\n","parkinson_images, parkinson_labels = load_dataset_with_limit(pd_folder,spectrogram_pd_folder, 'parkinson', target_count)\n","print(len(parkinson_images))\n","\n","# Combine datasets\n","images = healthy_images + parkinson_images\n","labels = healthy_labels + parkinson_labels"],"metadata":{"id":"EfVy4ZpApv4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into train and test sets\n","train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","# Create Hugging Face dataset structure\n","train_data = {\"image\": train_images, \"label\": train_labels}\n","test_data = {\"image\": test_images, \"label\": test_labels}\n","\n","# Convert to Hugging Face Dataset\n","train_dataset = Dataset.from_dict(train_data)\n","test_dataset = Dataset.from_dict(test_data)\n","\n","# Combine into a DatasetDict\n","dataset = DatasetDict({\n","    'train': train_dataset,\n","    'validation': test_dataset\n","})\n","\n","print(dataset)"],"metadata":{"id":"5o-ElgWIpv72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","labels = ['healthy', 'parkinson']\n","\n","label_map = {'healthy': 0, 'parkinson': 1}\n","\n","def collate_fn(batch):\n","    # Split the batch into images (X) and labels (y)\n","    # print(batch)\n","    images = [item['image'] for item in batch]  # First element in each item is the image\n","    labels = [item['label'] for item in batch]  # Second element in each item is the label\n","\n","    # Convert images to tensors\n","    images_tensor = torch.stack([torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in images])  # Convert images to tensors and change dimension order\n","\n","    # Convert labels from strings ('healthy', 'parkinson') to integers (0, 1)\n","    labels_tensor = torch.tensor([label_map[label] for label in labels], dtype=torch.long)\n","\n","    return {\n","        'pixel_values': images_tensor,  # Image tensor\n","        'labels': labels_tensor         # Label tensor (numerical)\n","    }"],"metadata":{"id":"N_WfpUNwqugw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoFeatureExtractor, MobileNetV2ForImageClassification, TrainingArguments, Trainer\n","from datasets import load_dataset\n","import numpy as np\n","import torch\n","from PIL import Image\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from torchvision.transforms import Compose, Normalize, Resize, ToTensor"],"metadata":{"id":"yjDmGbhNqujp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metrics for evaluation\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary', zero_division=0)\n","    acc = accuracy_score(labels, predictions)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","# Load a pre-trained MobileNetV2 model\n","model = MobileNetV2ForImageClassification.from_pretrained(\n","    \"google/mobilenet_v2_1.0_224\",\n","    num_labels=2,\n","    ignore_mismatched_sizes=True\n",")\n","\n","# Load a pre-trained feature extractor for MobileNetV2\n","feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n","\n","# Define transformations for images\n","transform = Compose([\n","    Resize((224, 224), antialias=True),\n","    ToTensor(),\n","    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n","])\n","\n","def preprocess(example):\n","    \"\"\"Process a single example.\"\"\"\n","    # Get the image from the example - using 'image' instead of 'pixel_values'\n","    image = example['image']\n","\n","    # Convert to numpy array if needed\n","    if isinstance(image, list):\n","        image = np.array(image, dtype=np.uint8)\n","\n","    # Handle different image formats\n","    if len(image.shape) == 4:\n","        image = np.squeeze(image, axis=(0, 1))\n","\n","    if len(image.shape) == 2:  # Grayscale\n","        image = np.stack([image] * 3, axis=-1)\n","    elif len(image.shape) == 3 and image.shape[0] == 3:  # Channels first\n","        image = np.transpose(image, (1, 2, 0))\n","\n","    # Convert to PIL and transform\n","    image = Image.fromarray(image)\n","    pixel_values = transform(image)\n","\n","    label_mapping = {'healthy': 0, 'parkinson': 1}\n","    label = label_mapping[example['label']]\n","\n","    # Return the processed example\n","    return {\n","        'pixel_values': pixel_values,\n","        'label': label\n","    }\n","\n","# Apply preprocessing to dataset\n","processed_dataset = dataset.map(\n","    preprocess,\n","    remove_columns=['image'],  # Remove only the 'image' column\n","    num_proc=4,\n","    desc=\"Processing images\"\n",")"],"metadata":{"id":"3kKbMwRPquoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data collator\n","def collate_fn(batch):\n","    # Convert image lists and labels into tensors directly\n","    pixel_values = torch.tensor([np.array(example[\"pixel_values\"]) for example in batch], dtype=torch.float32)\n","    labels = torch.tensor([example[\"label\"] for example in batch], dtype=torch.long)\n","\n","    return {\n","        \"pixel_values\": pixel_values,\n","        \"labels\": labels\n","    }\n","\n","# Test the collation\n","test_batch = [processed_dataset['train'][i] for i in range(4)]\n","test_collated = collate_fn(test_batch)\n","print(\"Collated batch shapes:\",\n","      \"\\nPixel values:\", test_collated[\"pixel_values\"].shape,\n","      \"\\nLabels:\", test_collated[\"labels\"].shape)"],"metadata":{"id":"_Bmb2jvjqut_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    eval_steps=80,\n","    save_steps=80,\n","    logging_steps=80,\n","    num_train_epochs=20,\n","    eval_strategy=\"steps\",\n","    save_total_limit=2,\n","    remove_unused_columns=False,\n","    report_to=\"tensorboard\",\n","    load_best_model_at_end=True,\n","    fp16=True,\n","    gradient_accumulation_steps=2,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    metric_for_best_model=\"f1\",\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=collate_fn,\n","    train_dataset=processed_dataset[\"train\"],\n","    eval_dataset=processed_dataset[\"validation\"],\n","    compute_metrics=compute_metrics\n",")"],"metadata":{"id":"xD4AQjC1quzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"I8rG-kABrRe8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/kaggle/working/MobileNet_model.pth')"],"metadata":{"id":"oH3SNwssrTs7"},"execution_count":null,"outputs":[]}]}