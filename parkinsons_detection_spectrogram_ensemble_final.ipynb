{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcBAGo3BrnuP"
      },
      "outputs": [],
      "source": [
        "!pip install librosa matplotlib numpy pillow\n",
        "!pip uninstall pyarrow -y\n",
        "!pip install --upgrade pyarrow datasets\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, DatasetDict\n",
        "import random\n",
        "\n",
        "# Define paths for the dataset\n",
        "path_to_data = '/kaggle/input/spectrograms-data/Spectrograms'\n",
        "hc_folder = os.path.join(path_to_data, 'HC_AH')  # Healthy audio samples\n",
        "pd_folder = os.path.join(path_to_data, 'PD_AH')  # Parkinson's audio samples\n",
        "\n",
        "# Paths for saving spectrograms\n",
        "spectrogram_hc_folder = os.path.join(path_to_data, 'healthy')\n",
        "spectrogram_pd_folder = os.path.join(path_to_data, 'parkinson')\n",
        "\n",
        "# Create directories if they do not exist\n",
        "os.makedirs(spectrogram_hc_folder, exist_ok=True)\n",
        "os.makedirs(spectrogram_pd_folder, exist_ok=True)\n",
        "\n",
        "def create_spectrogram(audio_path, save_folder, file_name, chunk_size=0.05, sample_rate=22050, limit=0):\n",
        "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
        "    chunk_length = int(chunk_size * sr)\n",
        "    total_chunks = len(y) // chunk_length\n",
        "    if limit == 0:\n",
        "        limit = total_chunks\n",
        "\n",
        "    for i in range(limit):\n",
        "        chunk = y[i * chunk_length: (i + 1) * chunk_length]\n",
        "        S = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64, n_fft=256, hop_length=64)\n",
        "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        plt.figure(figsize=(2, 2))\n",
        "        plt.axis('off')\n",
        "        librosa.display.specshow(S_DB, sr=sr, cmap='viridis')\n",
        "        save_path = os.path.join(save_folder, f'{file_name}_chunk_{i}.png')\n",
        "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "def is_folder_empty(folder_path):\n",
        "    return len(os.listdir(folder_path)) == 0\n",
        "\n",
        "for folder, label, save_folder in zip([hc_folder, pd_folder], ['healthy', 'parkinson'], [spectrogram_hc_folder, spectrogram_pd_folder]):\n",
        "    if is_folder_empty(save_folder):\n",
        "        print(f\"Generating spectrograms for {label} data...\")\n",
        "        for file in os.listdir(folder):\n",
        "            if file.endswith('.wav'):\n",
        "                file_path = os.path.join(folder, file)\n",
        "                create_spectrogram(file_path, save_folder, file_name=os.path.splitext(file)[0])\n",
        "    else:\n",
        "        print(f\"Spectrogram folder for {label} data already exists and is not empty. Skipping generation.\")\n",
        "\n",
        "def load_initial_images(spectrogram_folder, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for file in os.listdir(spectrogram_folder):\n",
        "        if file.endswith('.png'):\n",
        "            image_path = os.path.join(spectrogram_folder, file)\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            img = img.resize((224, 224))\n",
        "            images.append(np.array(img))\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "healthy_images, healthy_labels = load_initial_images(spectrogram_hc_folder, 'healthy')\n",
        "parkinson_images, parkinson_labels = load_initial_images(spectrogram_pd_folder, 'parkinson')\n",
        "\n",
        "target_count = min(len(healthy_images), len(parkinson_images))\n",
        "\n",
        "def load_dataset_with_limit(audio_folder, spectrogram_folder, label, target_count):\n",
        "    images, labels = load_initial_images(spectrogram_folder, label)\n",
        "    selected_indices = random.sample(range(len(images)), target_count)\n",
        "    images = [images[i] for i in selected_indices]\n",
        "    labels = [labels[i] for i in selected_indices]\n",
        "    return images, labels\n",
        "\n",
        "print(\"Count : \" + str(target_count))\n",
        "\n",
        "healthy_images, healthy_labels = load_dataset_with_limit(hc_folder, spectrogram_hc_folder, 'healthy', target_count)\n",
        "print(len(healthy_images))\n",
        "parkinson_images, parkinson_labels = load_dataset_with_limit(pd_folder, spectrogram_pd_folder, 'parkinson', target_count)\n",
        "print(len(parkinson_images))\n",
        "\n",
        "images = healthy_images + parkinson_images\n",
        "labels = healthy_labels + parkinson_labels\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = {\"image\": train_images, \"label\": train_labels}\n",
        "test_data = {\"image\": test_images, \"label\": test_labels}\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "test_dataset = Dataset.from_dict(test_data)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': test_dataset\n",
        "})\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "F3a5pofdzRIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from tqdm import tqdm\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"                        # the model name\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)   # load the image processor\n",
        "modelViT = ViTForImageClassification.from_pretrained(model_name)     # loading the pre-trained model\n",
        "\n",
        "labels = ['healthy', 'parkinson']\n",
        "\n",
        "label_map = {'healthy': 0, 'parkinson': 1}"
      ],
      "metadata": {
        "id": "9Ke0k23qsWhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Split the batch into images (X) and labels (y)\n",
        "    images = [item['image'] for item in batch]  # First element in each item is the image\n",
        "    labels = [item['label'] for item in batch]  # Second element in each item is the label\n",
        "\n",
        "    # Convert images to tensors\n",
        "    images_tensor = torch.stack([torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in images])  # Convert images to tensors and change dimension order\n",
        "\n",
        "    # Convert labels from strings ('healthy', 'parkinson') to integers (0, 1)\n",
        "    labels_tensor = torch.tensor([label_map[label] for label in labels], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'pixel_values': images_tensor,  # Image tensor\n",
        "        'labels': labels_tensor         # Label tensor (numerical)\n",
        "    }"
      ],
      "metadata": {
        "id": "qT1WOQK-sWlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # Predictions and true labels\n",
        "    predictions = eval_pred.predictions\n",
        "    references = eval_pred.label_ids\n",
        "\n",
        "    # Compute accuracy and F1 scores\n",
        "    accuracy_score = accuracy.compute(predictions=np.argmax(predictions, axis=1), references=references)\n",
        "    f1_score = f1.compute(predictions=np.argmax(predictions, axis=1), references=references, average=\"macro\")\n",
        "\n",
        "    # Compute AUC (for multiclass classification, use 'ovo' or 'ovr')\n",
        "    try:\n",
        "        auc_score = roc_auc_score(references, predictions, multi_class=\"ovr\", average=\"macro\")\n",
        "    except ValueError:\n",
        "        # Handle cases where AUC cannot be computed (e.g., single-class predictions)\n",
        "        auc_score = np.nan\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
        "        # Multiclass ROC curve\n",
        "        for i in range(predictions.shape[1]):\n",
        "            fpr, tpr, _ = roc_curve(references == i, predictions[:, i])\n",
        "            plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc_score(references == i, predictions[:, i]):.2f})\")\n",
        "    else:\n",
        "        # Binary ROC curve\n",
        "        fpr, tpr, _ = roc_curve(references, predictions[:, 1])\n",
        "        plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
        "\n",
        "    # Configure plot\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Return all metrics\n",
        "    return {**accuracy_score, **f1_score, \"auc\": auc_score}"
      ],
      "metadata": {
        "id": "vHWfg8_bs9X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "modelViT = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/kaggle/working/vit-base\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  eval_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=20,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=80,                # number of update steps before saving checkpoint\n",
        "  eval_steps=80,                # number of update steps before evaluating\n",
        "  logging_steps=80,             # number of update steps before logging\n",
        "  save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")"
      ],
      "metadata": {
        "id": "Ddnm2VNgtBMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainerViT = Trainer(\n",
        "    model=modelViT,                        # the instantiated Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=dataset[\"train\"],     # training dataset\n",
        "    eval_dataset=dataset[\"validation\"], # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")\n",
        "\n",
        "trainerViT.train()\n",
        "\n",
        "torch.save(modelViT.state_dict(), '/kaggle/working/ViT_model.pth')"
      ],
      "metadata": {
        "id": "1m7tbkIVtD5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet\n",
        "from transformers import AutoFeatureExtractor, ResNetForImageClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from tqdm import tqdm\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import accelerate\n",
        "import transformers"
      ],
      "metadata": {
        "id": "H20Y-5WktGes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['healthy', 'parkinson']\n",
        "\n",
        "label_map = {'healthy': 0, 'parkinson': 1}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Split the batch into images (X) and labels (y)\n",
        "    images = [item['image'] for item in batch]  # First element in each item is the image\n",
        "    labels = [item['label'] for item in batch]  # Second element in each item is the label\n",
        "\n",
        "    # Convert images to tensors\n",
        "    images_tensor = torch.stack([torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in images])  # Convert images to tensors and change dimension order\n",
        "\n",
        "    # Convert labels from strings ('healthy', 'parkinson') to integers (0, 1)\n",
        "    labels_tensor = torch.tensor([label_map[label] for label in labels], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'pixel_values': images_tensor,  # Image tensor\n",
        "        'labels': labels_tensor         # Label tensor (numerical)\n",
        "    }"
      ],
      "metadata": {
        "id": "Sq_8shw5tJjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # Predictions and true labels\n",
        "    predictions = eval_pred.predictions\n",
        "    references = eval_pred.label_ids\n",
        "\n",
        "    # Compute accuracy and F1 scores\n",
        "    accuracy_score = accuracy.compute(predictions=np.argmax(predictions, axis=1), references=references)\n",
        "    f1_score = f1.compute(predictions=np.argmax(predictions, axis=1), references=references, average=\"macro\")\n",
        "\n",
        "    # Compute AUC (for multiclass classification, use 'ovo' or 'ovr')\n",
        "    try:\n",
        "        auc_score = roc_auc_score(references, predictions, multi_class=\"ovr\", average=\"macro\")\n",
        "    except ValueError:\n",
        "        # Handle cases where AUC cannot be computed (e.g., single-class predictions)\n",
        "        auc_score = np.nan\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
        "        # Multiclass ROC curve\n",
        "        for i in range(predictions.shape[1]):\n",
        "            fpr, tpr, _ = roc_curve(references == i, predictions[:, i])\n",
        "            plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {roc_auc_score(references == i, predictions[:, i]):.2f})\")\n",
        "    else:\n",
        "            # Binary ROC curve\n",
        "        fpr, tpr, _ = roc_curve(references, predictions[:, 1])\n",
        "        plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
        "\n",
        "    # Configure plot\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Return all metrics\n",
        "    return {**accuracy_score, **f1_score, \"auc\": auc_score}"
      ],
      "metadata": {
        "id": "Ks6KdNEbtMDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelResNet = ResNetForImageClassification.from_pretrained(\n",
        "    'microsoft/resnet-50',\n",
        "    num_labels=2,  # Binary classification\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  eval_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=20,             # total number of training epochs\n",
        "  save_steps=80,                # number of update steps before saving checkpoint\n",
        "  eval_steps=80,                # number of update steps before evaluating\n",
        "  logging_steps=80,             # number of update steps before logging\n",
        "  save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,\n",
        "report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "7Z5_iR_TtRfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-trained ResNet feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-50')\n",
        "\n",
        "# Initializing the Trainer\n",
        "trainerResNet = Trainer(\n",
        "    model=modelResNet,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=feature_extractor,  # Use the feature extractor here\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainerResNet.train()\n",
        "\n",
        "torch.save(modelResNet.state_dict(), '/kaggle/working/ResNet_model.pth')"
      ],
      "metadata": {
        "id": "gx7j590MtWwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "labels = ['healthy', 'parkinson']\n",
        "\n",
        "label_map = {'healthy': 0, 'parkinson': 1}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Split the batch into images (X) and labels (y)\n",
        "    # print(batch)\n",
        "    images = [item['image'] for item in batch]  # First element in each item is the image\n",
        "    labels = [item['label'] for item in batch]  # Second element in each item is the label\n",
        "\n",
        "    # Convert images to tensors\n",
        "    images_tensor = torch.stack([torch.tensor(img, dtype=torch.float32).permute(2, 0, 1) for img in images])  # Convert images to tensors and change dimension order\n",
        "\n",
        "    # Convert labels from strings ('healthy', 'parkinson') to integers (0, 1)\n",
        "    labels_tensor = torch.tensor([label_map[label] for label in labels], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'pixel_values': images_tensor,  # Image tensor\n",
        "        'labels': labels_tensor         # Label tensor (numerical)\n",
        "    }"
      ],
      "metadata": {
        "id": "9-3h7ROdxsPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor, MobileNetV2ForImageClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torchvision.transforms import Compose, Normalize, Resize, ToTensor"
      ],
      "metadata": {
        "id": "NGPhXrGDxvqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary', zero_division=0)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Load a pre-trained MobileNetV2 model\n",
        "model = MobileNetV2ForImageClassification.from_pretrained(\n",
        "    \"google/mobilenet_v2_1.0_224\",\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Load a pre-trained feature extractor for MobileNetV2\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n",
        "\n",
        "# Define transformations for images\n",
        "transform = Compose([\n",
        "    Resize((224, 224), antialias=True),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "])\n",
        "\n",
        "def preprocess(example):\n",
        "    \"\"\"Process a single example.\"\"\"\n",
        "    # Get the image from the example - using 'image' instead of 'pixel_values'\n",
        "    image = example['image']\n",
        "\n",
        "    # Convert to numpy array if needed\n",
        "    if isinstance(image, list):\n",
        "        image = np.array(image, dtype=np.uint8)\n",
        "\n",
        "    # Handle different image formats\n",
        "    if len(image.shape) == 4:\n",
        "        image = np.squeeze(image, axis=(0, 1))\n",
        "\n",
        "    if len(image.shape) == 2:  # Grayscale\n",
        "        image = np.stack([image] * 3, axis=-1)\n",
        "    elif len(image.shape) == 3 and image.shape[0] == 3:  # Channels first\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "    # Convert to PIL and transform\n",
        "    image = Image.fromarray(image)\n",
        "    pixel_values = transform(image)\n",
        "\n",
        "    label_mapping = {'healthy': 0, 'parkinson': 1}\n",
        "    label = label_mapping[example['label']]\n",
        "\n",
        "    # Return the processed example\n",
        "    return {\n",
        "        'pixel_values': pixel_values,\n",
        "        'label': label\n",
        "    }\n",
        "\n",
        "# Apply preprocessing to dataset\n",
        "processed_dataset = dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=['image'],  # Remove only the 'image' column\n",
        "    num_proc=4,\n",
        "    desc=\"Processing images\"\n",
        ")"
      ],
      "metadata": {
        "id": "_Z_XAlShxyg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "def collate_fn(batch):\n",
        "    # Convert image lists and labels into tensors directly\n",
        "    pixel_values = torch.tensor([np.array(example[\"pixel_values\"]) for example in batch], dtype=torch.float32)\n",
        "    labels = torch.tensor([example[\"label\"] for example in batch], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "# Test the collation\n",
        "test_batch = [processed_dataset['train'][i] for i in range(4)]\n",
        "test_collated = collate_fn(test_batch)\n",
        "print(\"Collated batch shapes:\",\n",
        "      \"\\nPixel values:\", test_collated[\"pixel_values\"].shape,\n",
        "      \"\\nLabels:\", test_collated[\"labels\"].shape)"
      ],
      "metadata": {
        "id": "j9Ny4KJbx1sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    eval_steps=80,\n",
        "    save_steps=80,\n",
        "    logging_steps=80,\n",
        "    num_train_epochs=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    metric_for_best_model=\"f1\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    eval_dataset=processed_dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "T6S7vNp8x1v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "P2x4yf9qx6YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/kaggle/working/MobileNet_model.pth')"
      ],
      "metadata": {
        "id": "mjervLSkx8H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "import torch\n",
        "\n",
        "labels = ['healthy', 'parkinson']\n",
        "\n",
        "# Re-initialize the model architecture with correct label mappings\n",
        "modelViT_loaded = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    num_labels=2,\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# Load the saved weights\n",
        "modelViT_loaded.load_state_dict(torch.load(\"/kaggle/input/vit_parkinson/pytorch/default/1/ViT_model.pth\", map_location=torch.device(\"cpu\")))"
      ],
      "metadata": {
        "id": "tPe2_R4cyDMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from transformers import AutoFeatureExtractor\n",
        "\n",
        "# Load a pre-trained feature extractor for MobileNetV2\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "])"
      ],
      "metadata": {
        "id": "35HNIAxHyDUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ResNetForImageClassification\n",
        "\n",
        "# Recreate the model architecture\n",
        "modelResNet_loaded = ResNetForImageClassification.from_pretrained(\n",
        "    'microsoft/resnet-50',\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# Load the saved weights into the model\n",
        "modelResNet_loaded.load_state_dict(torch.load(\"/kaggle/input/resnet_parkinson/pytorch/default/1/ResNet_model.pth\", map_location=torch.device(\"cpu\")))"
      ],
      "metadata": {
        "id": "RN0esLRtyDbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MobileNetV2ForImageClassification\n",
        "\n",
        "# Re-initialize the model architecture\n",
        "modelMobileNet_loaded = MobileNetV2ForImageClassification.from_pretrained(\n",
        "    \"google/mobilenet_v2_1.0_224\",\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Load the state_dict\n",
        "modelMobileNet_loaded.load_state_dict(torch.load(\"/kaggle/input/mobilenet_parkinson/pytorch/default/1/MobileNet_model.pth\", map_location=torch.device(\"cpu\")))"
      ],
      "metadata": {
        "id": "jNdNy-D2yDeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "modelResNet_loaded = modelResNet_loaded.to(device)\n",
        "modelViT_loaded = modelViT_loaded.to(device)\n",
        "modelMobileNet_loaded = modelMobileNet_loaded.to(device)\n",
        "\n",
        "basic_transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "NKNT6iaxyDhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ensemble(image):\n",
        "    x1 = transform(image).unsqueeze(0).to(device)\n",
        "    x = basic_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        r = modelResNet_loaded(x)\n",
        "        v = modelViT_loaded(x)\n",
        "        m = modelMobileNet_loaded(x1)\n",
        "\n",
        "    # Average predictions\n",
        "    avg_logits = (r.logits + v.logits + m.logits) / 3\n",
        "    pred = torch.argmax(avg_logits, dim=1).item()\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "-we0iYo_yNwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for example in tqdm(dataset[\"validation\"]):\n",
        "    raw_image = example['image']\n",
        "\n",
        "    # Ensure image is in HWC format for PIL conversion\n",
        "    if isinstance(raw_image, list):\n",
        "        raw_image = np.array(raw_image, dtype=np.uint8)\n",
        "\n",
        "    if isinstance(raw_image, torch.Tensor):\n",
        "        raw_image = raw_image.permute(1, 2, 0).numpy()  # CHW → HWC\n",
        "\n",
        "    if len(raw_image.shape) == 2:  # Grayscale\n",
        "        raw_image = np.stack([raw_image] * 3, axis=-1)\n",
        "    elif len(raw_image.shape) == 3 and raw_image.shape[0] == 3:  # CHW\n",
        "        raw_image = np.transpose(raw_image, (1, 2, 0))  # Convert to HWC\n",
        "\n",
        "    pil_image = Image.fromarray(raw_image.astype(np.uint8))\n",
        "\n",
        "    # Don't transform here. Let predict_ensemble handle it.\n",
        "    pred = predict_ensemble(pil_image)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    if example['label'] == \"healthy\":\n",
        "        true_labels.append(0)\n",
        "    else:\n",
        "        true_labels.append(1)"
      ],
      "metadata": {
        "id": "MIFSybEEyNyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "print(true_labels[0])\n",
        "print(predictions[0])\n",
        "print(classification_report(true_labels, predictions, target_names=[\"healthy\", \"parkinson\"]))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
        "print(\"F1 Score:\",f1_score(true_labels,predictions))"
      ],
      "metadata": {
        "id": "t6OhOQKzyN1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ensemble_hardvote(image):\n",
        "    x1 = transform(image).unsqueeze(0).to(device)\n",
        "    x = basic_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get predicted class from each model (0 or 1)\n",
        "        r_pred = torch.argmax(modelResNet_loaded(x).logits, dim=1).item()\n",
        "        v_pred = torch.argmax(modelViT_loaded(x).logits, dim=1).item()\n",
        "        m_pred = torch.argmax(modelMobileNet_loaded(x1).logits, dim=1).item()\n",
        "\n",
        "    # Majority voting\n",
        "    preds = [r_pred, v_pred, m_pred]\n",
        "    final_pred = round(sum(preds) / len(preds))  # returns 1 if 2 or more are 1, else 0\n",
        "\n",
        "    return final_pred"
      ],
      "metadata": {
        "id": "Vm4tRMKSyN4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "for example in tqdm(dataset[\"validation\"]):\n",
        "    raw_image = example['image']\n",
        "\n",
        "    # Ensure image is in HWC format for PIL conversion\n",
        "    if isinstance(raw_image, list):\n",
        "        raw_image = np.array(raw_image, dtype=np.uint8)\n",
        "\n",
        "    if isinstance(raw_image, torch.Tensor):\n",
        "        raw_image = raw_image.permute(1, 2, 0).numpy()  # CHW → HWC\n",
        "\n",
        "    if len(raw_image.shape) == 2:  # Grayscale\n",
        "        raw_image = np.stack([raw_image] * 3, axis=-1)\n",
        "    elif len(raw_image.shape) == 3 and raw_image.shape[0] == 3:  # CHW\n",
        "        raw_image = np.transpose(raw_image, (1, 2, 0))  # Convert to HWC\n",
        "\n",
        "    pil_image = Image.fromarray(raw_image.astype(np.uint8))\n",
        "\n",
        "    # Don't transform here. Let predict_ensemble handle it.\n",
        "    pred = predict_ensemble_hardvote(pil_image)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    if example['label'] == \"healthy\":\n",
        "        true_labels.append(0)\n",
        "    else:\n",
        "        true_labels.append(1)"
      ],
      "metadata": {
        "id": "IXPE42vWyXcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing using hardVoting\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "print(true_labels[0])\n",
        "print(predictions[0])\n",
        "print(classification_report(true_labels, predictions, target_names=[\"healthy\", \"parkinson\"]))\n",
        "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
        "print(\"F1 Score:\",f1_score(true_labels,predictions))"
      ],
      "metadata": {
        "id": "503sB-_JybaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}